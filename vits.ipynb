{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hparams import load_hparams_json\n",
    "from utils.util import intersperse\n",
    "import json\n",
    "from models.synthesizer.models.vits import Vits\n",
    "import torch\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from models.synthesizer.utils.symbols import symbols\n",
    "from models.synthesizer.utils.text import text_to_sequence\n",
    "\n",
    "\n",
    "hps = load_hparams_json(\"data/ckpt/synthesizer/vits2/config.json\")\n",
    "print(hps.train)\n",
    "model = Vits(\n",
    "    len(symbols),\n",
    "    hps[\"data\"][\"filter_length\"] // 2 + 1,\n",
    "    hps[\"train\"][\"segment_size\"] // hps[\"data\"][\"hop_length\"],\n",
    "    n_speakers=hps[\"data\"][\"n_speakers\"],\n",
    "    **hps[\"model\"])\n",
    "_ = model.eval()\n",
    "device = torch.device(\"cpu\")\n",
    "checkpoint = torch.load(str(\"data/ckpt/synthesizer/vits2/G_120000.pth\"), map_location=device)\n",
    "if \"model_state\" in checkpoint:\n",
    "    state = checkpoint[\"model_state\"]\n",
    "else:\n",
    "    state = checkpoint[\"model\"]\n",
    "model.load_state_dict(state, strict=False)\n",
    "\n",
    "# 随机抽取情感参考音频的根目录\n",
    "random_emotion_root = \"D:\\\\audiodata\\\\aidatatang_200zh\\\\corpus\\\\train\\\\G0017\"\n",
    "import random, re\n",
    "from pypinyin import lazy_pinyin, Style\n",
    "\n",
    "import os\n",
    "\n",
    "def tts(txt, emotion, sid=0):\n",
    "    txt = \" \".join(lazy_pinyin(txt, style=Style.TONE3, neutral_tone_with_five=True))\n",
    "    text_norm = text_to_sequence(txt, hps[\"data\"][\"text_cleaners\"])\n",
    "    if hps[\"data\"][\"add_blank\"]:\n",
    "        text_norm = intersperse(text_norm, 0)\n",
    "    stn_tst = torch.LongTensor(text_norm)\n",
    "\n",
    "    with torch.no_grad(): #inference mode\n",
    "        x_tst = stn_tst.unsqueeze(0)\n",
    "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)])\n",
    "        sid = torch.LongTensor([sid])\n",
    "        if emotion.endswith(\"wav\"):\n",
    "            from models.synthesizer.preprocess_audio import extract_emo\n",
    "            import librosa\n",
    "            wav, sr = librosa.load(emotion, 16000)\n",
    "            emo = torch.FloatTensor(extract_emo(np.expand_dims(wav, 0), sr, embeddings=True))\n",
    "        else:\n",
    "            print(\"emotion参数不正确\")\n",
    "\n",
    "        audio = model.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=0.667, noise_scale_w=0.8, length_scale=1, emo=emo)[0][0,0].data.float().numpy()\n",
    "    ipd.display(ipd.Audio(audio, rate=hps[\"data\"][\"sampling_rate\"], normalize=False))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"随机抽取的音频文件路径可以用于使用该情感合成其他句子\"\n",
    "tts(txt, emotion='C:\\\\Users\\\\babys\\\\Desktop\\\\voicecollection\\\\secondround\\\\美玉.wav', sid=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.synthesizer.preprocess import preprocess_dataset\n",
    "from pathlib import Path\n",
    "from utils.hparams import HParams\n",
    "datasets_root = Path(\"../audiodata/\")\n",
    "hparams = HParams(\n",
    "        n_fft = 1024, # filter_length\n",
    "        num_mels = 80,\n",
    "        hop_size = 256,                             # Tacotron uses 12.5 ms frame shift (set to sample_rate * 0.0125)\n",
    "        win_size = 1024,                             # Tacotron uses 50 ms frame length (set to sample_rate * 0.050)\n",
    "        fmin = 55,\n",
    "        min_level_db = -100,\n",
    "        ref_level_db = 20,\n",
    "        max_abs_value = 4.,                         # Gradient explodes if too big, premature convergence if too small.\n",
    "        sample_rate = 16000,\n",
    "        rescale = True,\n",
    "        max_mel_frames = 900,\n",
    "        rescaling_max = 0.9,        \n",
    "        preemphasis = 0.97,                         # Filter coefficient to use if preemphasize is True\n",
    "        preemphasize = True,\n",
    "        ### Mel Visualization and Griffin-Lim\n",
    "        signal_normalization = True,\n",
    "\n",
    "        utterance_min_duration = 1.6,               # Duration in seconds below which utterances are discarded\n",
    "        ### Audio processing options\n",
    "        fmax = 7600,                                # Should not exceed (sample_rate // 2)\n",
    "        allow_clipping_in_normalization = True,     # Used when signal_normalization = True\n",
    "        clip_mels_length = True,                    # If true, discards samples exceeding max_mel_frames\n",
    "        use_lws = False,                            # \"Fast spectrogram phase recovery using local weighted sums\"\n",
    "        symmetric_mels = True,                      # Sets mel range to [-max_abs_value, max_abs_value] if True,\n",
    "                                                    #               and [0, max_abs_value] if False\n",
    "        trim_silence = True,                        # Use with sample_rate of 16000 for best results\n",
    "\n",
    ")\n",
    "preprocess_dataset(datasets_root=datasets_root, \n",
    "        out_dir=datasets_root.joinpath(\"SV2TTS\", \"synthesizer\"),\n",
    "        n_processes=8,\n",
    "        skip_existing=True, \n",
    "        hparams=hparams, \n",
    "        no_alignments=False, \n",
    "        dataset=\"magicdata\", \n",
    "        emotion_extract=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.synthesizer.train_vits import run\n",
    "from pathlib import Path\n",
    "from utils.hparams import HParams\n",
    "import torch, os\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "datasets_root = Path(\"../audiodata/SV2TTS/synthesizer\")\n",
    "hparams= HParams(\n",
    "  model_dir = \"data/ckpt/synthesizer/vits\",\n",
    ")\n",
    "hparams.loadJson(Path(hparams.model_dir).joinpath(\"config.json\"))\n",
    "hparams.data[\"training_files\"] = str(datasets_root.joinpath(\"train.txt\"))\n",
    "hparams.data[\"validation_files\"] = str(datasets_root.joinpath(\"train.txt\"))\n",
    "hparams.data[\"datasets_root\"] = str(datasets_root)\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "# for spawn\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '8899'\n",
    "mp.spawn(run, nprocs=n_gpus, args=(n_gpus, hparams))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "挑选只有对应emo文件的meta数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "root = Path('../audiodata/SV2TTS/synthesizer')\n",
    "dict_info = []\n",
    "with open(root.joinpath(\"train.txt\"), \"r\", encoding=\"utf-8\") as dict_meta:\n",
    "    for raw in dict_meta:\n",
    "        if not raw:\n",
    "            continue\n",
    "        v = raw.split(\"|\")[0].replace(\"audio\",\"emo\")\n",
    "        emo_fpath = root.joinpath(\"emo\").joinpath(v)\n",
    "        if emo_fpath.exists():\n",
    "            dict_info.append(raw)\n",
    "        # else:\n",
    "        #     print(emo_fpath)\n",
    "# Iterate over each wav\n",
    "meta2 = Path('../audiodata/SV2TTS/synthesizer/train2.txt')\n",
    "metadata_file = meta2.open(\"w\", encoding=\"utf-8\")\n",
    "for new_info in dict_info:\n",
    "    metadata_file.write(new_info)\n",
    "metadata_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "emo_root = Path('../audiodata/SV2TTS/synthesizer').joinpath('emo')\n",
    "# raw_root = Path('../audiodata/aidatatang_200zh/corpus/train')\n",
    "# emo_file_list = emo_root.glob(\"**/*.npy\")\n",
    "# for emo_file in emo_file_list:\n",
    "#     if emo_file.name.endswith('wav__00.npy'):\n",
    "#         folder = emo_file.parent\n",
    "#         os.rename(emo_file, folder.joinpath(emo_file.name.replace(\"__00\", \"_00\")))\n",
    "    # shutil.move(emo_file, emo_root.joinpath(emo_file.name))\n",
    "\n",
    "root = Path('../audiodata/SV2TTS/synthesizer')\n",
    "dict_info = []\n",
    "with open(root.joinpath(\"train.txt\"), \"r\", encoding=\"utf-8\") as dict_meta:\n",
    "    for raw in dict_meta:\n",
    "        if not raw:\n",
    "            continue\n",
    "        v = raw.split(\"|\")[0].replace(\"audio\",\"emo\")\n",
    "        emo_fpath = root.joinpath(\"emo\").joinpath(v)\n",
    "        if emo_fpath.exists():\n",
    "            dict_info.append(raw)\n",
    "        # else:\n",
    "        #     print(emo_fpath)\n",
    "# Iterate over each wav\n",
    "meta2 = Path('../audiodata/SV2TTS/synthesizer/train2.txt')\n",
    "metadata_file = meta2.open(\"w\", encoding=\"utf-8\")\n",
    "for new_info in dict_info:\n",
    "    metadata_file.write(new_info)\n",
    "metadata_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "788ab866da3baa6c99886d56abb59fe71b6a552bf52c65473ecf96c784704db8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
